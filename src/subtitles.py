from faster_whisper.transcribe import Segment, Word
from transcribe import transcribe, TranscribeOptions
from filters import compile_filters, filter_transcription
import ffmpeg
import os, pathlib, argparse
from cli import confirm
from dataclasses import dataclass
import math

@dataclass
class Subtitle:
  start: float
  end: float
  words: list[Word]

def layout_subtitles(segments: list[Segment], respect_segments: bool) -> list[Subtitle]:
  """Transforms a list of transcription segments into a list of subtitles."""
  # Conventions to follow: https://engagemedia.org/help/best-practices-for-online-subtitling/
  # - Max = 2 lines
  # - In general, appear and disappear with timing of spoken text.
  # - Minimum time = max(1.5 seconds, long enough to read), unless next line needs to appear sooner.
  # - If a line is repeated, insert a blank gap between the repeats.
  # - Separate subtitle for each sentence, except for very short sentences.
  # - Prefer line break on punctuation.
  # More conventions: https://diposit.ub.edu/dspace/bitstream/2445/128428/1/subtitling%20and%20captioning.pdf
  # - 40 characters per line
  # - >=1.5, <=6 seconds per subtitle
  # - 0.125 second gap between subtitles
  sentences = [s.words for s in segments] if respect_segments else _group_sentences(segments)
  subtitles = []
  subtitle_char_count = 0
  subtitle_words = []
  for sentence in sentences:
    if len(subtitle_words) > 0:
      start = subtitle_words[0].start
      next_start = sentence[0].start
      if start + 1.5 < next_start - 0.125:
        end = max(start + 1.5, subtitle_words[-1].end)
        end = min(end, next_start - 0.125)
        subtitles.append(Subtitle(start, end, subtitle_words))
        subtitle_words = []
        subtitle_char_count = 0

    for word in sentence:
      if subtitle_char_count + len(word.word) > 80:
        subtitles.append(Subtitle(subtitle_words[0].start, word.start, subtitle_words))
        subtitle_words = []
        subtitle_char_count = 0

      subtitle_words.append(word)
      subtitle_char_count += len(word.word)
    # TODO: Skip subtitles/sentences/phrases with low average probability
  
  if len(subtitle_words) > 0:
    start = subtitle_words[0].start
    end = max(start + 1.5, subtitle_words[-1].end)
    subtitles.append(Subtitle(start, end, subtitle_words))

  return subtitles

def _group_sentences(segments: list[Segment]) -> list[list[Word]]:
  """Groups words from a list into sentences based on punctuation."""
  sentences = []
  current_sentence = []
  for segment in segments:
    for word in segment.words:
      current_sentence.append(word)
      text = word.word
      if text.endswith('.') or text.endswith('!') or text.endswith('?'):
        sentences.append(current_sentence)
        current_sentence = []

  if len(current_sentence) > 0:
    sentences.append(current_sentence)

  return sentences

FILE_HEADER = \
"""[Script Info]
; Script generated by automute
ScriptType: v4.00+
PlayResX: 480
PlayResY: 360
ScaledBorderAndShadow: yes

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Arial,18,&Hffffff,&H00ffff,&H000000,&H80000000,0,0,0,0,100,100,0,0,1,1,1,2,10,10,10,0

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
LINE_FORMAT = "Dialogue: 0,{start},{end},Default,,0,0,0,,{text}\n"
WORD_FORMAT = "{{\\1a&H{alpha:02X}&}}{text}{{\\1a}}"

# SRT does not seem to support font opacity, only color.
# mov_text (i.e., mp4) does not seem to support font color at all.
# Only SubStation Alpha seems to support opacity.
# It seems like ffmpeg likes to have all of the possible fields present, at least when converting from SSA to SRT.
def create_subtitles_script(subtitles: list[Subtitle]) -> str:
  contents = [FILE_HEADER]
  for subtitle in subtitles:
    start = _seconds_to_ts(subtitle.start)
    end = _seconds_to_ts(subtitle.end)
    text = ""
    first = True
    for word in subtitle.words:
      word_text = word.word
      if first:
        word_text = word_text.lstrip()
        first = False
      text += WORD_FORMAT.format(alpha=_probability_to_alpha(word.probability), text=word_text)
    contents.append(LINE_FORMAT.format(start=start, end=end, text=text.strip()))
  return ''.join(contents)

def _seconds_to_ts(seconds: float) -> str:
  centiseconds = round(seconds * 100)
  seconds = centiseconds // 100
  minutes = seconds // 60
  hours = minutes // 60
  centiseconds %= 100
  seconds %= 60
  minutes %= 60
  return f"{hours:01}:{minutes:02}:{seconds:02}.{centiseconds:02}"

def _probability_to_alpha(probability: float) -> int:
  MIN_ALPHA = 0x00
  MAX_ALPHA = 0xA0
  MIN_PROBABILITY = 0.2
  MAX_PROBABILITY = 0.9
  lerp = 1.0 - min(max((probability - MIN_PROBABILITY) / (MAX_PROBABILITY - MIN_PROBABILITY), 0.0), 1.0)
  return MIN_ALPHA + int(lerp * (MAX_ALPHA - MIN_ALPHA))

def add_subtitles_to_video(video_file: str, subtitles_script: str, output_file: str):
  if os.path.isfile(output_file):
    if confirm("Output file already exists. Overwrite it?", default=False):
      os.remove(output_file)
    else:
      print("Canceled")
      exit(1)
  av = ffmpeg.input(video_file)
  subtitles = ffmpeg.input('pipe:')
  stream = ffmpeg.output(av, subtitles, output_file, c="copy", loglevel="error")
  process = ffmpeg.run_async(stream, pipe_stdin=True)
  process.stdin.write(subtitles_script.encode())
  process.stdin.close()
  if process.wait() != 0:
    print("Error from ffmpeg")

def _parse_arguments() -> argparse.Namespace:
  parser = argparse.ArgumentParser(
    prog='subtitles',
    description='A command-line tool for adding subtitles to automute-filtered videos.'
  )
  parser.add_argument('input',
                      help='Audio or video file to add subtitles to.')
  parser.add_argument('-o', '--output',
                      help='Name of the output file. (Default: <input file>-subtitled.mkv)')
  parser.add_argument('-w', '--filter-word', default=[], action='append',
                      help='A word to filter out. Treated as a case-insensitive regular expression. Can be specified multiple times.')
  parser.add_argument('-f', '--filter-file', default=[], action='append',
                      help="A file of words to filter out. Each line is treated as a case-insentive regular expression.")
  parser.add_argument('-e', '--encipher-words', default=False, action='store_true',
                      help='Before applying filters, enchiper transcribed words by replacing each letter with the one immediately after it in the alphabet ' +
                           '(looping around at the end; i.e., caesar cipher with a shift of 1). Use this if you need to filter out profanity but would prefer ' +
                           'not to read profanity when specifying your filters.')
  parser.add_argument('--whisper-model', default='large-v3',
                      help='The faster_whisper model to use for audio transcription. Can be a model name (tiny, tiny.en, base, base.en, small, small.en, ' +
                           'distil-small.en, medium, medium.en, distil-medium.en, large, large-v1, large-v2, distil-large-v2, large-v3, or distil-large-v3), ' +
                           'a CTranslate2-converted model ID from Hugging Face, or a path to a local model. See faster-whisper docs. (Default: large-v3)')
  parser.add_argument('--whisper-compute-type', default='auto', choices=['default','auto','int8','int8_float16','int8_bfloat16','int8_float32','int16','float16','bfloat16','float32'],
                      help='The compute type to use when loading the Whisper model. \'default\' explicitly uses the same quantization that the model is already using. ' + 
                           '\'auto\' selects the fasted option that is supported by the device used. (Default: auto)')
  parser.add_argument('--whisper-device', default='auto', choices=['auto','cpu','cuda'],
                      help='The compute device to use when running the Whisper model. (Default: auto)')
  parser.add_argument('--respect-segments', default=False, action='store_true',
                      help="Use the segment boundaries from the Whisper transcription. This is sometimes useful for videos that primarily contain lyrics.")
  # parser.add_argument('--whisper-silence-ms', default=-1, type=int,
  #                     help='The minimum duration in milliseconds for an audio segment with no detected speech to be skipped during transcription. -1 to disable. ' +
  #                          '(Default: -1)')
  parser.add_argument('--min-logprob', default=-1.2, type=float,
                      help="The minimum average log probabilty that a transcription segment must have to be included in subtitles. (Default: -1.2)")
  parser.add_argument('--ignore-cached-transcriptions', default=False, action='store_true',
                      help='Ignore any cached transcriptions.')
  return parser.parse_args()

def _get_filtered_video_path(input_file: str) -> str:
  """Creates an output file path from an input file path."""
  input_path = pathlib.Path(input_file)
  return str(input_path.with_stem(input_path.stem + "-filtered"))

def _get_output_file_path(input_file: str) -> str:
  """Creates an output file path from an input file path."""
  input_path = pathlib.Path(input_file)
  return str(input_path.with_stem(input_path.stem + "-subtitled").with_suffix(".mkv"))

def _avg_word_log_prob(segment: Segment) -> float:
  return sum(math.log10(word.probability) for word in segment.words) / len(segment.words)

def main():
  args = _parse_arguments()

  input_file = args.input
  filtered_file = _get_filtered_video_path(input_file)
  output_file = args.output if args.output is not None else _get_output_file_path(input_file)
  
  filters = compile_filters(args.filter_word, args.filter_file)

  segments = transcribe(
    input_file,
    TranscribeOptions(
      model=args.whisper_model,
      device=args.whisper_device,
      compute_type=args.whisper_compute_type,
      condition_on_previous_text='distil' not in args.whisper_model, # Distil models seem prone to repeating themselves
      # hotwords=[decipher(word) if args.encipher_words else word for f in filters for word in [f.pattern[2:-2]]],
      # vad_filter=args.whisper_silence_ms >= 0,
      # vad_parameters=dict(
      #   min_silence_duration_ms=args.whisper_silence_ms
      # ) if args.whisper_silence_ms >= 0 else dict()
    ),
    ignore_cache=args.ignore_cached_transcriptions,
  )
  min_logprob = args.min_logprob
  segments = [s for s in segments if s.avg_logprob >= min_logprob and _avg_word_log_prob(s) >= min_logprob]

  segments, matches = filter_transcription(segments, filters, '[__]', args.encipher_words)
  if len(filters) > 0:
    print(f"Found {matches} matches for filters")

  subtitles = layout_subtitles(segments, respect_segments=args.respect_segments)
  script = create_subtitles_script(subtitles)

  add_subtitles_to_video(filtered_file, script, output_file)
  print("Done")

if __name__ == "__main__":
  main()
